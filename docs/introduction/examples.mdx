---
title: 'Examples'
description: 'Quick snippets showing some features of Enochian'
---

## Backend Swapping

If you want to do mixture of agents or something along those lines, you can dynamically swap between backends.

```typescript
import ProgramState, { OpenAIBackend } from 'enochian-js';

const s = new ProgramState();

await s.setModel('http://localhost:30000');
await s
    .add(s.system`You are a helpful assistant.`)
    .add(s.user`Tell me a joke`)
    .add(s.assistant`${s.gen('answer1')}`);

s.setBackend(
    new OpenAIBackend({ apiKey: process.env.OPENAI_KEY }),
).setModel({ modelName: 'gpt-4o' });

await s
    .add(s.user`Tell me a better one`)
    .add(s.assistant`No problem! ${s.gen('answer2')}`);
console.log(s.get('answer1'), '\n------\n', s.get('answer2'));
```

## Choices

Force the LLM to pick between multiple choices. It's implemented by implemented by computing the [token-length normalized log probabilities](https://blog.eleuther.ai/multiple-choice-normalization/) of all choices and selecting the one with the highest probability (full credit goes to the sglang folks for this).

```typescript
import ProgramState from 'enochian-js';

const s = new ProgramState();

await s.setModel('http://localhost:30000');
await s
    .add(s.system`You are an animal.`)
    .add(s.user`What are you?`)
    .add(
        s.assistant`I am a ${s.gen('answer', { choices: ['hippopotamus', 'giraffe'] })}`,
    );

console.log(s.get('answer'));
```

## Forking

Sometimes, you can reach a state in your program which is parallelizable.

```typescript
import ProgramState from 'enochian-js';

const s = new ProgramState();
await s.setModel('http://localhost:30000');

s.add(s.system`You are a helpful assistant.`)
    .add(s.user`How can I stay healthy?`)
    .add(s.assistant`Here are two tips for staying healthy:
        1. Balanced Diet. 2. Regular Exercise.\n\n`);

const forks = s.fork(2);
await Promise.all(
    forks.map((f, i) =>
        f
            .add(
                f.user`Now, expand tip ${(i + 1).toString()} into a paragraph.`,
            )
            .add(
                f.assistant`${f.gen('detailed_tip', { sampling_params: { max_new_tokens: 256 } })}`,
            ),
    ),
);

await s
    .add(s.user`Please expand.`)
    .add(s.assistant`Tip 1: ${forks[0]?.get('detailed_tip') ?? ''}
        Tip 2: ${forks[1]?.get('detailed_tip') ?? ''}
        In summary, ${s.gen('summary')}`);

console.log(s.get('summary'));
```

In this case, the expanded tips for 1 and 2 don't depend on each other, so we can generate both at the same time using the `fork` function.
Then, we can join them back into the parent `ProgramState` and continue generating from there.

## Streaming

Enochian of course also supports streaming, so you can use it for a chat application frontend.

```typescript
import ProgramState from 'enochian-js';

const s = new ProgramState();

await s.setModel('http://localhost:30000');
const gen = s
    .add(s.system`You are a helpful assistant.`)
    .add(s.user`Tell me a joke`)
    .add(s.assistant`${s.gen('answer1', { stream: true })}`);
for await (const chunk of gen) {
    console.log(chunk.content);
}
console.log(s.get('answer1'), s.getMetaInfo('answer1'));
```

## Constrained decoding

For structured outputs, you can pass in either a regex, a [JsonSchema](https://json-schema.org/), or a
[zod](https://zod.dev/) schema inside [`SamplingParams`](/api-reference/request-types#SamplingParams).
Zod types are the preferred way since it allows for type safety in the [`.get`](/api-reference/program-state#get).

```typescript
import ProgramState from 'enochian-js'
import { z } from 'zod';

const schema = z.object({
    id: z.string().uuid(),
    name: z.string().min(2).max(50),
    age: z.number().int().min(0).max(120),
    email: z.string().email(),
    tags: z.array(z.string()).min(1).max(5),
    role: z.enum(['admin', 'user', 'guest']),
    settings: z
        .object({
            theme: z.enum(['light', 'dark']),
            notifications: z.boolean(),
        })
        .optional(),
    // You can't use z.date() since the LLM will generate a datestring, not a Date
    joinedAt: z.string().date(),
});

const s = new ProgramState();

await s.setModel('http://localhost:30000');
await s
    .add(s.user`Describe a google employee's profile in json format`)
    .add(
        s.assistant`${s.gen('answer', { sampling_params: { zod_schema: schema } })}`,
    );
const profile = s.get('answer', { with: 'zod', schema });
console.log(profile);
```

## Tool Calling

You can provide the LLM some tools that it can optionally use per generation. You should use the
`createTools` util which will allow for greater type inference when retrieving the result.

```typescript
import ProgramState, { OpenAIBackend, createTools } from 'enochian-js';
import { z } from 'zod';

const getWeatherParams = z.object({
    location: z.string()
});
function getWeather(args: z.infer<typeof getWeatherParams>) {
    return "It's cold and rainy";
}
const tools = createTools([{
    function: getWeather,
    name: 'getWeather',
    description: 'Gets the current weather at some location',
    params: getWeatherParams,
}]);

const s = new ProgramState();
await s.setModel('http://localhost:30000');
await s
    .add(s.user`What's the weather like in Boston?`)
    .add(s.assistant`${s.gen('action', { tools )}`);

const action = s.get('action', { from: 'tools', tools });
console.log(action[0]);
```

It will return a [`ToolUseResponse`](/api-reference/program-state#ToolUseResponse) typed object:
```typescript
type ToolUseResponse = ({
    toolUsed: string;
    response: unknown;
} | {
    toolUsed: "respondToUser";
    response: string;
})[]
```

The LLM can either call one or multiple tools OR respond to the user but not both. The reason you
want to use the `createTools` util is because the `ToolUseResponse` type cannot infer the literal
value of `toolUsed` and the `response` type without it. With the `createTools` util, hovering over
`action` in your IDE should show something like:

```typescript
const action: ({
    toolUsed: "respondToUser";
    response: string;
} | {
    toolUsed: "getWeather";
    respond: string;
})[]
```

The LLM can either directly respond to the user or call the `getWeather` function and
enochian is able to infer that.

If you wanted to build a weather-getting agent, you could do a very simple "game loop"
like this:


```typescript
while (true) {
    await s.add(
        s.assistant`${gen('action', {
            tools,
        })}`,
    );
    const action = s.get('action', { from: 'tools', tools });
    if (!action) {
        console.error(`No tool was used: ${action}`);
        return;
    }
    const respondToUserAction = action.find(a => a.toolUsed === 'respondToUser');
    if (respondToUserAction) {
        console.log(respondToUserAction.response);
        return;
    } else {
        for (const toolUsed of action) {
            s.add(
                s.user`${toolUsed.toolUsed} Result: ${JSON.stringify(toolUsed.response)}`,
            );
        }
    }
}
```

That will repeatedly call the LLM, giving it the responses to its tool uses, until
it decides it can finally directly respond to the user.
